{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_phrasebank = load_dataset('financial_phrasebank', 'sentences_50agree') \n",
    "financial_phrasebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Examples: \n",
    "- 0: 'negative'\n",
    "- 1: 'neutral'\n",
    "- 2: 'positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sentence:\", financial_phrasebank['train'][0]['sentence'])\n",
    "print(\"Label:\", financial_phrasebank['train'][0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sentence:\", financial_phrasebank['train'][3]['sentence'])\n",
    "print(\"Label:\", financial_phrasebank['train'][3]['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_inputs, remain_inputs, train_labels, remain_labels = train_test_split(financial_phrasebank['train']['sentence'], \n",
    "                                                                            financial_phrasebank['train']['label'], \n",
    "                                                                            train_size=0.8)\n",
    "\n",
    "test_inputs, validation_inputs, test_labels, validation_labels = train_test_split(remain_inputs, remain_labels, test_size=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_phrasebank_dict = {\n",
    "    'train': {'sentence': train_inputs, \n",
    "              'label': train_labels\n",
    "              },\n",
    "    'valid': {'sentence': validation_inputs, \n",
    "              'label': validation_labels\n",
    "              },\n",
    "    'test': {'sentence': test_inputs, \n",
    "              'label': test_labels\n",
    "             }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_phrasebank_dict['train']['sentence'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer \n",
    "# tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\", do_lower_case=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sentence:\", financial_phrasebank['train']['sentence'][0])\n",
    "print(\"Tokens:\", tokenizer.tokenize(financial_phrasebank['train']['sentence'][0]))\n",
    "print(\"Token IDs:\", tokenizer.convert_tokens_to_ids(tokenizer.tokenize(financial_phrasebank['train']['sentence'][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize and add paddings to all of the sentences and map the tokens to thier word IDs.\n",
    "For every sentence encode will:\n",
    "  - (1) Tokenize the sentence.\n",
    "  - (2) Prepend the `[CLS]` token to the start. - token id 101\n",
    "  - (3) Append the `[SEP]` token to the end. - token id 102\n",
    "  - (4) Map tokens to their IDs.\n",
    "  - (5) Ensure all sentences are equal length. Pad sequences with 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_phrasebank_dict['train'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_datasets(data):\n",
    "  tokenized_datasets = {}\n",
    "  for collection in data: \n",
    "    tokenized_datasets[collection] = tokenizer(data[collection]['sentence'], padding='max_length', max_length = 64, truncation=True, return_tensors='pt')\n",
    "    tokenized_datasets[collection]['label'] = data[collection]['label']\n",
    "  \n",
    "  return tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenize_datasets(financial_phrasebank_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets['train'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets['train']['input_ids'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Max train sentence length: ', max([len(sen) for sen in financial_phrasebank_dict['train']['sentence']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_datasets['valid']['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset and dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(batch_size, tokenized_data):\n",
    "  #Creating the DataLoader which will help us to load data into the GPU/CPU\n",
    "  batch_size = batch_size\n",
    "  dataloaders = {}\n",
    "  # Create the DataLoader for our data set.\n",
    "  for collection in tokenized_data: \n",
    "    data = TensorDataset(tokenized_data[collection]['input_ids'], tokenized_data[collection]['token_type_ids'], \n",
    "                         tokenized_data[collection]['attention_mask'], torch.tensor(tokenized_data[collection]['label']))\n",
    "    if collection == 'train':\n",
    "      sampler = RandomSampler(data)\n",
    "    else: \n",
    "      sampler = SequentialSampler(data)\n",
    "    \n",
    "    dataloaders[collection] = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "  return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders = data_loader(batch_size=16, tokenized_data=tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the pre-trained BERT model from huggingface library: \n",
    "BertForSequenceClassification the pretrained BERT model with a single linear classification layer on top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-pretrain',num_labels=3)\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 3)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert',num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def accuracy(preds, labels): \n",
    "  preds = preds.detach().cpu().numpy()\n",
    "  labels = labels.to('cpu').numpy()\n",
    "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "  labels_flat = labels.flatten()\n",
    "  return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# def accuracy(out, labels):\n",
    "#     out = out.cpu().numpy()\n",
    "#     labels = labels.to('cpu').numpy()\n",
    "#     outputs = np.argmax(out, axis=1)\n",
    "#     return np.sum(outputs == labels) / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_dataloader, val_dataloader, num_epochs, load_pretrained=False):\n",
    "    \n",
    "    plot_cache = {'train_loss':[], 'train_acc': [], 'val_loss':[], 'val_acc': []}\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"\")\n",
    "        print(\"Epoch:\", epoch)\n",
    "        if not load_pretrained:\n",
    "            \n",
    "            model.train() \n",
    "            counter = 0\n",
    "            \n",
    "            train_batch_loss = 0\n",
    "            train_batch_acc = 0\n",
    "            \n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                optimizer.zero_grad()\n",
    "                counter += 1\n",
    "\n",
    "                b_input_ids = batch[0].to(device)\n",
    "                b_token_type_ids = batch[1].to(device)\n",
    "                b_input_mask = batch[2].to(device)\n",
    "                b_labels = batch[3].to(device)\n",
    "\n",
    "                logits = model(b_input_ids, token_type_ids=b_token_type_ids,attention_mask=b_input_mask)[0]\n",
    "                # print(logits.size())   \n",
    "                # print(b_labels.size())\n",
    "                loss = criterion(logits.view(-1, logits.size()[1]), b_labels.view(-1))\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                #train batch accuracy & loss: \n",
    "                train_batch_loss += loss.item()\n",
    "                train_batch_acc += accuracy(logits, b_labels)\n",
    "\n",
    "            epoch_train_loss = train_batch_loss / counter\n",
    "            train_losses.append(epoch_train_loss)\n",
    "\n",
    "            epoch_train_acc = train_batch_acc / counter\n",
    "            train_accs.append(epoch_train_acc)\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"  Average training loss: {0:.2f}\".format(epoch_train_loss))\n",
    "            print(\"  Average training acc: {0:.2f}\".format(epoch_train_acc))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            val_batch_loss = 0\n",
    "            val_batch_acc = 0 \n",
    "            val_counter = 0 \n",
    "\n",
    "            for step, batch in enumerate(val_dataloader):\n",
    "                val_counter +=1 \n",
    "                b_input_ids = batch[0].to(device)\n",
    "                b_token_type_ids = batch[1].to(device)\n",
    "                b_input_mask = batch[2].to(device)\n",
    "                b_labels = batch[3].to(device)\n",
    "\n",
    "                logits = model(b_input_ids, token_type_ids=b_token_type_ids,attention_mask=b_input_mask)[0]\n",
    "                        \n",
    "                val_loss = criterion(logits.view(-1, logits.size()[1]), b_labels.view(-1))\n",
    "                \n",
    "                #validation batch accuracy & loss: \n",
    "                val_batch_loss += val_loss.item()\n",
    "                val_batch_acc += accuracy(logits, b_labels)\n",
    "                # print(\"Batch acc:\", val_batch_acc)\n",
    "                # print(\"Batch loss:\", val_batch_loss)\n",
    "\n",
    "            epoch_val_loss = val_batch_loss / val_counter\n",
    "            val_losses.append(epoch_val_loss)\n",
    "\n",
    "            epoch_val_acc = val_batch_acc / val_counter\n",
    "            val_accs.append(epoch_val_acc)\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"  Average validation loss: {0:.2f}\".format(epoch_val_loss))\n",
    "            print(\"  Average validation accuracy: {0:.2f}\".format(epoch_val_acc))\n",
    "\n",
    "        plot_cache['val_loss'].append(epoch_val_loss)\n",
    "        plot_cache['val_acc'].append(epoch_val_acc)\n",
    "\n",
    "        plot_cache['train_loss'].append(epoch_train_loss)\n",
    "        plot_cache['train_acc'].append(epoch_train_acc)\n",
    "    \n",
    "    return plot_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cache = train(model.to(device), optimizer, criterion, train_dataloader=data_loaders['train'], val_dataloader=data_loaders['valid'], num_epochs=5,  load_pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating our model on the test set\n",
    "\n",
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(data_loaders['test'])))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "pred_labels , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in data_loaders['test']:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  #Unpack the inputs from our dataloader\n",
    "  b_input_ids,b_token_type_ids,  b_input_mask, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  pred_labels.append(logits)\n",
    "  true_labels.append(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: 'negative'\n",
    "# 1: 'neutral'\n",
    "# 2: 'positive\n",
    "neutral, positive, negative = 0,0,0\n",
    "for num in financial_phrasebank_dict['test']['label']: \n",
    "  if num == 1:\n",
    "    neutral += 1 \n",
    "  elif num == 2: \n",
    "    positive += 1 \n",
    "  else: \n",
    "    negative += 1\n",
    "\n",
    "test_size = len(financial_phrasebank_dict['test']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (positive, test_size, (positive / test_size * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of one batch of predictions:' , len(pred_labels[0]), '\\n' , pred_labels[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of one batch of true labels:' , len(true_labels[0]), '\\n' , true_labels[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each input batch the predictions are a 3-column ndarray (one column for \"0\", one column for \"1\", and one column for \"2\"). \n",
    "# Pick the label with the highest value and turn this\n",
    "predictions = []\n",
    "for i in range(len(pred_labels)):\n",
    "  predictions.append(np.argmax(pred_labels[i], axis=1).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = np.concatenate(true_labels, axis =0)\n",
    "predictions = np.concatenate(predictions, axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy\", accuracy_score(actual, predictions))\n",
    "print(\"Precision\", precision_score(actual, predictions, average='macro'))\n",
    "print(\"F1 Score\", f1_score(actual, predictions, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refrences: \n",
    " - https://huggingface.co/FinanceInc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, optimizer, criterion, train_dataloader, num_epochs, load_pretrained=False):\n",
    "#     plot_cache = {'train_loss':[], 'train_acc': [], 'val_loss':[], 'val_acc': []}\n",
    "#     train_losses = []\n",
    "#     train_accs = []\n",
    "#     val_losses = []\n",
    "#     val_accs = []\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         print(\"Epoch:\", epoch)\n",
    "#         if not load_pretrained:\n",
    "#           model.train() \n",
    "#             counter = 0\n",
    "#             train_batch_loss = 0\n",
    "#             train_batch_acc = 0\n",
    "\n",
    "#             val_batch_loss = 0\n",
    "#             val_batch_acc = 0 \n",
    "\n",
    "            \n",
    "#             for step, batch in enumerate(train_dataloader):\n",
    "                \n",
    "#                 b_input_ids = batch[0].to(device)\n",
    "#                 b_token_type_ids = batch[1].to(device)\n",
    "#                 b_input_mask = batch[2].to(device)\n",
    "#                 b_labels = batch[3].to(device)\n",
    "\n",
    "#                 optimizer.zero_grad()\n",
    "#                 counter += 1\n",
    "\n",
    "#                 logits = model(b_input_ids, token_type_ids=b_token_type_ids,attention_mask=b_input_mask)[0]\n",
    "#                 # print(logits.size())   \n",
    "#                 # print(b_labels.size())\n",
    "#                 loss = criterion(logits.view(-1, logits.size()[1]), b_labels.view(-1))\n",
    "#                 train_batch_loss += loss.item()\n",
    "\n",
    "#                 #train batch accuracy: \n",
    "#                 train_batch_acc += accuracy(logits, b_labels)\n",
    "\n",
    "#                 loss.backward()\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#                 optimizer.step()\n",
    "            \n",
    "#             epoch_train_loss = train_batch_loss / counter\n",
    "#             print(counter)\n",
    "#             train_losses.append(epoch_train_loss)\n",
    "\n",
    "#             epoch_train_acc = train_batch_acc / counter\n",
    "#             # print(counter)\n",
    "#             train_accs.append(epoch_train_acc)\n",
    "\n",
    "#             print(\"\")\n",
    "#             print(\"  Average training loss: {0:.2f}\".format(epoch_train_loss))\n",
    "#             print(\"  Average training acc: {0:.2f}\".format(epoch_train_acc))\n",
    "\n",
    "#         plot_cache['train_loss'].append(epoch_train_loss)\n",
    "#         plot_cache['train_acc'].append(epoch_train_acc)\n",
    "    \n",
    "#     return plot_cache, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cache, model = train(model.to(device), optimizer, criterion, train_dataloader=data_loaders['train'], num_epochs=5, load_pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 1\n",
    "\n",
    "# val_losses = []\n",
    "# val_accs = []\n",
    "# plot_cache = {'val_loss':[], 'val_acc': []}\n",
    "\n",
    "\n",
    "# for epoch in range(num_epochs): \n",
    "#   print(\"Epoch:\", epoch)\n",
    "#   val_batch_loss = 0\n",
    "#   val_batch_acc = 0 \n",
    "    \n",
    "#   counter = 0 \n",
    "#   model.eval()\n",
    "  \n",
    "#   with torch.no_grad():\n",
    "#     for step, batch in enumerate(data_loaders['valid']):\n",
    "#       counter += 1\n",
    "#       # print(\"Iteration:\", step)\n",
    "#       b_input_ids = batch[0].to(device)\n",
    "#       b_token_type_ids = batch[1].to(device)\n",
    "#       b_input_mask = batch[2].to(device)\n",
    "#       b_labels = batch[3].to(device)\n",
    "\n",
    "#       logits = model(b_input_ids, token_type_ids=b_token_type_ids,attention_mask=b_input_mask)[0]\n",
    "            \n",
    "#       val_loss = criterion(logits.view(-1, logits.size()[1]), b_labels.view(-1))\n",
    "#       val_batch_loss += val_loss.item()\n",
    "#       # print(type(val_batch_loss))\n",
    "\n",
    "#       #batch accuracy \n",
    "#       val_batch_acc += accuracy(logits, b_labels)\n",
    "#       print(\"Batch acc:\", val_batch_acc)\n",
    "#       print(\"Batch loss:\", val_batch_loss)\n",
    "\n",
    "#       # print(type(val_batch_acc))\n",
    "\n",
    "      \n",
    "\n",
    "#     # epoch_val_loss = val_batch_loss / counter\n",
    "#     # print(counter)\n",
    "#     # val_losses.append(epoch_val_loss)\n",
    "\n",
    "#     epoch_val_acc = val_batch_acc / counter\n",
    "#     # # print(counter)\n",
    "#     # val_accs.append(epoch_val_acc)\n",
    "\n",
    "#     # print(\"\")\n",
    "#     # print(\"  Average validation loss: {0:.2f}\".format(epoch_val_loss))\n",
    "#     print(\"  Average validation accuracy: {0:.2f}\".format(epoch_val_acc))\n",
    "\n",
    "#     # plot_cache['val_loss'].append(epoch_val_loss)\n",
    "#     # plot_cache['val_acc'].append(epoch_val_acc)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
